{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Hands-on: LLM Inference Optimization with Batching and Caching\n","\n","**The Challenge:** Deploying Large Language Models (LLMs) in production is often bottlenecked by high latency and prohibitive GPU costs. Every second spent waiting for a response costs money and degrades user experience.\n","\n","**The Solution:** This notebook tears down the barriers to efficient LLM serving by implementing and benchmarking two crucial, performance-boosting techniques: Inference Batching and intelligent Query Caching.\n","\n","Let's dive in and unlock the hidden performance of your LLM infrastructure!"],"metadata":{"id":"yeLR3z7EObRh"}},{"cell_type":"markdown","source":["## Setup\n","\n","This initial step securely imports your Hugging Face token (assumed to be stored in a service like Google Colab's user data secrets) and logs you in, ensuring the Qwen model can be downloaded and used for the subsequent optimization benchmarks."],"metadata":{"id":"oYiLfsEOPArT"}},{"cell_type":"code","source":["from google.colab import userdata\n","from huggingface_hub import login\n","import torch\n","import time\n","from transformers import AutoTokenizer, AutoModelForCausalLM"],"metadata":{"id":"JJMi7K3KP8CQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hf_token = userdata.get('HF_TOKEN')\n","login(hf_token, add_to_git_credential=True)"],"metadata":{"id":"viOJUnqG80-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Core Setup: Initializing the Qwen LLM\n","We are using the Qwen/Qwen2.5-1.5B-Instruct model, a compact yet powerful decoder-only LLM, loaded in torch.float16 precision for efficient GPU usage. The generate_single function establishes our baseline by simulating a standard user request and processing one prompt at a time, providing a reference for performance. Subsequent optimizations, such as batched inference and semantic caching, will be compared against this sequential baseline to evaluate improvements in latency and throughput."],"metadata":{"id":"VpPBZ2DMPgq_"}},{"cell_type":"code","source":["MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.padding_side = \"left\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bilsr-YCJv1o","executionInfo":{"status":"ok","timestamp":1764179223275,"user_tz":-120,"elapsed":15601,"user":{"displayName":"Mariam Hussein","userId":"05184323440420360102"}},"outputId":"299d7592-34a7-4cc3-b8ea-bf1d0a301c15"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(151936, 1536)\n","    (layers): ModuleList(\n","      (0-27): 28 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n","          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n","          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n","          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n","          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n","        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n","    (rotary_emb): Qwen2RotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["# Single prompt generation\n","def generate_single(prompt):\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    text = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n","    )\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n","\n","    generated_ids = model.generate(\n","        **model_inputs,\n","        max_new_tokens=512\n","    )\n","    generated_ids = [\n","        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    return response"],"metadata":{"id":"3gkR4mmsJt3P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference Batching\n","Inference batching is fundamental to modern LLM serving. Instead of running prompts one after another (leaving the GPU idle between requests), we bundle multiple prompts into a single payload. This maximizes the utilization of the high-throughput parallel processing power of the GPU, leading to massive gains in requests per second (throughput) and a dramatic drop in effective latency per user. We will compare sequential inference (batch size 1) against true batch processing."],"metadata":{"id":"ufxAlcGWP_9K"}},{"cell_type":"code","source":["def generate_batch(prompts, max_new_tokens=512):\n","    # Step 1: Apply chat template to each prompt\n","    texts = [\n","        tokenizer.apply_chat_template(\n","            [{\"role\": \"user\", \"content\": p}],\n","            tokenize=False,\n","            add_generation_prompt=True\n","        )\n","        for p in prompts\n","    ]\n","\n","    # Step 2: Tokenize all prompts as a batch\n","    model_inputs = tokenizer(\n","        texts,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    ).to(model.device)\n","\n","    # Step 3: Generate outputs\n","    with torch.no_grad():\n","        outputs = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n","\n","    # Step 4: Extract generated tokens per prompt\n","    results = []\n","    for i in range(len(prompts)):\n","        # Correctly skip prompt tokens using attention_mask\n","        seq_len = model_inputs['attention_mask'][i].sum()\n","        generated_tokens = outputs[i][seq_len:]\n","        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","        results.append(decoded)\n","\n","    return results"],"metadata":{"id":"OJYzibRb_3Bc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Comparison of Sequential vs. Batched Latency**\n","The test below demonstrates how combining multiple LLM calls into a single batch can drastically cut down overall processing time by maximizing GPU utilization."],"metadata":{"id":"-5oogJm_QuZa"}},{"cell_type":"code","source":["\n","prompts = [\n","    \"Explain quantum computing in simple terms.\",\n","    \"What are the benefits of meditation?\",\n","    \"Give me a summary of climate change effects.\",\n","    \"Define reinforcement learning.\"\n","]\n","\n","start = time.time()\n","single_results = [generate_single(p) for p in prompts]\n","end = time.time()\n","\n","print(f\"\\n=== Single Prompt Latency ===\")\n","print(f\"Total time for {len(prompts)} prompts: {end - start:.3f}s\")\n","print(f\"Average per prompt: {(end - start)/len(prompts):.3f}s\\n\")\n","\n","start = time.time()\n","batch_results = generate_batch(prompts)\n","end = time.time()\n","\n","print(f\"=== Batched Latency ===\")\n","print(f\"Total batch time: {end - start:.3f}s\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJXJFeLoQX8N","executionInfo":{"status":"ok","timestamp":1764179287476,"user_tz":-120,"elapsed":64188,"user":{"displayName":"Mariam Hussein","userId":"05184323440420360102"}},"outputId":"9efecda4-bc2f-4d4e-a9d2-be9bcc9a45c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Single Prompt Latency ===\n","Total time for 4 prompts: 48.334s\n","Average per prompt: 12.083s\n","\n","=== Batched Latency ===\n","Total batch time: 15.777s\n"]}]},{"cell_type":"markdown","source":["## Query Caching"],"metadata":{"id":"VSFGpqHAFzAY"}},{"cell_type":"markdown","source":["### Exact Caching\n","\n","For highly repetitive queries, such as FAQ systems or internal developer tools, running the LLM multiple times is pure waste. Exact-match caching uses a simple dictionary lookup to store the result of a function call based on its arguments. This provides near-zero latency for repeated, identical inputs, completely bypassing the massive computational cost of the LLM forward pass."],"metadata":{"id":"CBZ9ltpBF1Jk"}},{"cell_type":"code","source":["from functools import lru_cache\n","import time\n","\n","# Simple exact-match cache\n","@lru_cache(maxsize=128)\n","def generate_cached(prompt):\n","    return generate_single(prompt)\n","\n","prompt = \"Define chaching.\"\n","\n","# ---- Cold cache ----\n","start = time.time()\n","cold_output = generate_cached(prompt)\n","cold_time = time.time() - start\n","\n","print(\"Cold output:\", cold_output)\n","print(f\"Cold cache latency: {cold_time:.3f}s\\n\")\n","\n","# ---- Warm cache ----\n","start = time.time()\n","warm_output = generate_cached(prompt)\n","warm_time = time.time() - start\n","\n","print(\"Warm output:\", warm_output)\n","print(f\"Warm cache latency: {warm_time:.6f}s \")\n"],"metadata":{"id":"tpdkY9Oe_55_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764179298742,"user_tz":-120,"elapsed":11263,"user":{"displayName":"Mariam Hussein","userId":"05184323440420360102"}},"outputId":"3efc1052-fd37-4ac5-9ab5-dab7ada074fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cold output: Cache is a type of data storage that stores frequently accessed data to speed up access times and reduce the load on main memory or disk drives. It is used in computer systems, networks, and other computing environments where performance can be improved by reducing the time required to retrieve information from slower storage devices.\n","\n","In more technical terms, caching involves storing copies of recently accessed data in a faster, more efficient form (such as in RAM instead of hard disk) so that subsequent requests for that same data do not need to be retrieved again from the slower source. This reduces the overall processing time and increases system responsiveness.\n","\n","Caches can be implemented at various levels within a computer system, including:\n","\n","1. Hardware-level caches: These are built into the processor's hardware and store frequently used instructions and data.\n","2. Operating System level caches: These are managed by the operating system and store data that is frequently accessed by applications running on the system.\n","3. Application-level caches: These are maintained by individual application programs and store frequently used data specific to their operations.\n","\n","Caches play a crucial role in optimizing the performance of large-scale computing systems, databases, web servers, and other resource-intensive applications. They help to improve response times, reduce latency, and enhance overall efficiency by minimizing the amount of data that needs to be fetched from slower storage media like hard disks or SSDs.\n","Cold cache latency: 11.330s\n","\n","Warm output: Cache is a type of data storage that stores frequently accessed data to speed up access times and reduce the load on main memory or disk drives. It is used in computer systems, networks, and other computing environments where performance can be improved by reducing the time required to retrieve information from slower storage devices.\n","\n","In more technical terms, caching involves storing copies of recently accessed data in a faster, more efficient form (such as in RAM instead of hard disk) so that subsequent requests for that same data do not need to be retrieved again from the slower source. This reduces the overall processing time and increases system responsiveness.\n","\n","Caches can be implemented at various levels within a computer system, including:\n","\n","1. Hardware-level caches: These are built into the processor's hardware and store frequently used instructions and data.\n","2. Operating System level caches: These are managed by the operating system and store data that is frequently accessed by applications running on the system.\n","3. Application-level caches: These are maintained by individual application programs and store frequently used data specific to their operations.\n","\n","Caches play a crucial role in optimizing the performance of large-scale computing systems, databases, web servers, and other resource-intensive applications. They help to improve response times, reduce latency, and enhance overall efficiency by minimizing the amount of data that needs to be fetched from slower storage media like hard disks or SSDs.\n","Warm cache latency: 0.000064s \n"]}]},{"cell_type":"markdown","source":["### Sematic Caching\n","\n","This method goes beyond exact matching. It understands the meaning of the query, making it the most powerful caching strategy for user-facing applications. If a user rephrases a question (e.g., \"What is RL?\" instead of \"Define reinforcement learning.\"), the cache still detects the semantic similarity and returns the instant, stored response. This requires an embedding model to convert text into comparable vectors.\n","\n","**How it works:**\n","1. Embeddings: Queries are converted into numerical vectors (embeddings).\n","2. Cosine Similarity: The similarity between the incoming query's vector and all cached vectors is calculated.\n","3. Cache Hit: If the cosine similarity score $\\ge 0.8$, it's considered a semantic match, and the cached response is returned."],"metadata":{"id":"LgvVYBUEF48S"}},{"cell_type":"markdown","source":["**Setup & Embedding Model Initialization**"],"metadata":{"id":"kRaM35EzH5N6"}},{"cell_type":"code","source":["!pip install -q sentence-transformers numpy\n","\n","import time\n","from sentence_transformers import SentenceTransformer\n","import numpy as np\n","import json\n","\n","# Global list to store queries, embeddings, and responses\n","semantic_cache = []\n","\n","# Initialize the embedding model (small and fast model, suitable for caching)\n","# We choose a fast model to ensure the embedding process doesn't negate the latency savings.\n","embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","print(\" Libraries installed and embedding model initialized.\")\n","print(f\"Embedding dimensions: {embedding_model.get_sentence_embedding_dimension()}\")"],"metadata":{"id":"tCTjxRfQAcjK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764179308166,"user_tz":-120,"elapsed":9420,"user":{"displayName":"Mariam Hussein","userId":"05184323440420360102"}},"outputId":"4a5ee334-2cba-4690-bb29-5c6c62190c07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Libraries installed and embedding model initialized.\n","Embedding dimensions: 384\n"]}]},{"cell_type":"markdown","source":["**1. Define Core Cache Logic (`query` and `add` functions)**\n","\n","These functions manage the cache:\n","* `semantic_cache_query`: Calculates the similarity score between the new query and every cached item. The similarity calculation uses the **dot product** of normalized vectors, which is equivalent to **cosine similarity**.\n","* `add_to_semantic_cache`: Converts the new query into an embedding and stores it with the response."],"metadata":{"id":"2cHwaMN5IGCF"}},{"cell_type":"code","source":["def semantic_cache_query(query, threshold=0.8):\n","    # Embed the incoming query\n","    query_emb = embedding_model.encode([query])[0]\n","\n","    for item in semantic_cache:\n","        # Cosine Similarity Calculation: (A . B) / (||A|| * ||B||)\n","        sim = np.dot(query_emb, item['embedding']) / (np.linalg.norm(query_emb) * np.linalg.norm(item['embedding']))\n","\n","        if sim >= threshold:\n","            print(f\"\\n[CACHE HIT!] Similarity: {sim:.4f}\")\n","            return item['response']\n","\n","    return None\n","\n","def add_to_semantic_cache(query, response):\n","    query_emb = embedding_model.encode([query])[0]\n","    semantic_cache.append({'query': query, 'embedding': query_emb, 'response': response})\n","    print(\"[Cache added]\")"],"metadata":{"id":"f8AamE7aAeMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.Define Generation with semantic cache**"],"metadata":{"id":"mBm7lDpdItEi"}},{"cell_type":"code","source":["def generate_with_semantic_cache(query):\n","    cached = semantic_cache_query(query)\n","    if cached:\n","        return cached\n","\n","    # Call your LLM\n","    response = generate_single(query)\n","    add_to_semantic_cache(query, response)\n","    return response\n"],"metadata":{"id":"jFwNkgH0AfyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Test and Results: Cold vs. Warm Cache**\n","\n","We define a set of queries designed to trigger a semantic hit on the second run:\n","* Query 1 and Query 2 (\"Explain RL\" vs. \"What is RL?\") are semantically similar and should hit the cache.\n","* Query 3 is new and acts as a baseline.\n","\n","We will run the queries twice:\n","1.  **Cold Cache:** All queries miss, resulting in high latency.\n","2.  **Warm Cache:** Subsequent semantically similar queries hit, resulting in near-zero latency."],"metadata":{"id":"n0SCV9F_Ih6L"}},{"cell_type":"code","source":["queries = [\n","    \"Explain reinforcement learning.\",        # Q1 (Miss, Cache: 1)\n","    \"Describe benefits of meditation.\",        # Q2 (Miss, Cache: 2)\n","    \"What is reinforcement learning?\",         # Q3 (Hit Q1)\n","    \"Tell me about the advantages of meditating.\" # Q4 (Hit Q2)\n","]\n","\n","print(\"--- COLD CACHE RUN---\")\n","start_cold = time.time()\n","for q in queries:\n","    out = generate_with_semantic_cache(q)\n","    print(f\"Query: {q}\\nResponse: {out[:70]}...\\n\")\n","end_cold = time.time()\n","print(f\"Cold cache total latency: {end_cold - start_cold:.3f}s\")\n","\n","\n","print(\"\\n--- WARM CACHE RUN---\")\n","start_warm = time.time()\n","for q in queries:\n","    out = generate_with_semantic_cache(q)\n","    # Print the cache status from the function\n","    print(f\"Query: {q}\\nResponse: {out[:70]}...\\n\")\n","end_warm = time.time()\n","print(f\"Warm cache total latency: {end_warm - start_warm:.6f}s\")"],"metadata":{"id":"-At-mAhKAhy5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764179364436,"user_tz":-120,"elapsed":56253,"user":{"displayName":"Mariam Hussein","userId":"05184323440420360102"}},"outputId":"7e3edb15-0e4f-4450-8efc-26d9dae857e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- COLD CACHE RUN---\n","[Cache added]\n","Query: Explain reinforcement learning.\n","Response: Reinforcement Learning (RL) is an area of machine learning that focuse...\n","\n","[Cache added]\n","Query: Describe benefits of meditation.\n","Response: Meditation has numerous benefits for both physical and mental health. ...\n","\n","\n","[CACHE HIT!] Similarity: 0.8803\n","Query: What is reinforcement learning?\n","Response: Reinforcement Learning (RL) is an area of machine learning that focuse...\n","\n","\n","[CACHE HIT!] Similarity: 0.8332\n","Query: Tell me about the advantages of meditating.\n","Response: Meditation has numerous benefits for both physical and mental health. ...\n","\n","Cold cache total latency: 56.302s\n","\n","--- WARM CACHE RUN---\n","\n","[CACHE HIT!] Similarity: 1.0000\n","Query: Explain reinforcement learning.\n","Response: Reinforcement Learning (RL) is an area of machine learning that focuse...\n","\n","\n","[CACHE HIT!] Similarity: 1.0000\n","Query: Describe benefits of meditation.\n","Response: Meditation has numerous benefits for both physical and mental health. ...\n","\n","\n","[CACHE HIT!] Similarity: 0.8803\n","Query: What is reinforcement learning?\n","Response: Reinforcement Learning (RL) is an area of machine learning that focuse...\n","\n","\n","[CACHE HIT!] Similarity: 0.8332\n","Query: Tell me about the advantages of meditating.\n","Response: Meditation has numerous benefits for both physical and mental health. ...\n","\n","Warm cache total latency: 0.022723s\n"]}]}]}