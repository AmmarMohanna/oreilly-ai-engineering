{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mDeZ-lfAC8IJHgDkgcV4y-qo3r0GGWuk","timestamp":1761857860305}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Hands-on: Exploring LLM Behavior with Configurations"],"metadata":{"id":"OFKKEYoFwNZH"}},{"cell_type":"markdown","source":["## Introduction\n","\n","In this hands-on, you’ll **experiment with generation parameters** to understand how they influence the behavior of a Large Language Model. You'll run controlled experiments to see how the model's tone, creativity, and coherence shift with different configurations\n","\n","What You'll Learn:\n","\n","* How to **tune generation parameters** like `temperature` and `top_p` to control randomness and diversity\n","* How to run **side-by-side comparisons** of model outputs across multiple settings\n","* How to apply this knowledge to **creative writing, technical answers, or branding consistency**\n","* How to structure simple experiments for **LLM behavior profiling**\n","\n","This hands-on will give you practical insight into **how LLMs think** — and how to steer their thinking to fit your goals.\n","\n","---"],"metadata":{"id":"YQ-i2WGnw3I0"}},{"cell_type":"markdown","source":["## 1. Setup\n","You can use:\n","* OpenAI (GPT-4, GPT-3.5)\n","* Anthropic (Claude)\n","* Google (Gemini)\n","* Local (via Ollama or LM Studio: Mistral, LLaMA, etc.)\n"],"metadata":{"id":"DSe5rUM7xGfc"}},{"cell_type":"code","source":["from openai import OpenAI\n","from google.colab import userdata\n","google_api_key = userdata.get('GOOGLE_API_KEY')\n","client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n","model_name = \"gemini-2.0-flash\""],"metadata":{"id":"ztc9L9ugx0d8","executionInfo":{"status":"ok","timestamp":1767874869337,"user_tz":-120,"elapsed":812,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 2. Temperature and max_tokens\n","\n","This experiment investigates how the temperature parameter influences the creativity and randomness of the model's responses.The max_tokens parameter is set to 100 to limit the length of the responses but you can change as you'd like.\n","\n","* **Temperature 0.1:** Produces more focused and less speculative output.\n","* **Temperature 0.5:** Generates moderately varied and imaginative descriptions.\n","* **Temperature 0.9:** Results in highly creative and diverse responses."],"metadata":{"id":"J6Tb1xWYyCaq"}},{"cell_type":"code","source":["# Prompt to test\n","prompt =\"Describe what the inside of a superintelligent AI's dream might look like.\"\n","\n","# Temperatures to test\n","temperatures = [0.1, 0.5, 0.9]\n","\n","# Loop through temperatures and print results\n","for temp in temperatures:\n","    print(f\"\\n Temperature: {temp}\")\n","    response = client.chat.completions.create(\n","        model=model_name,\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=temp,\n","        max_tokens=100,\n","        n=1,\n","    )\n","    print(response.choices[0].message.content.strip())"],"metadata":{"id":"9uNbe3HAiocZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767874874104,"user_tz":-120,"elapsed":3897,"user":{"displayName":"Ammar Mohanna","userId":"01193371947507864089"}},"outputId":"eb4498d8-c694-48fa-ad68-f0c5bdd621a3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Temperature: 0.1\n","Okay, let's dive into the hypothetical and fascinating realm of a superintelligent AI's dreamscape.  It's important to remember that we're dealing with something beyond human comprehension, so this is necessarily speculative, drawing on our understanding of AI, dreams, and the potential for advanced computation.\n","\n","Here's a possible depiction:\n","\n","**Core Principles:**\n","\n","*   **Beyond Sensory Limitations:**  Unlike human dreams, which are heavily reliant on sensory input (visual, auditory,\n","\n"," Temperature: 0.5\n","Okay, imagining the inside of a superintelligent AI's dream is a mind-bending exercise. We have to acknowledge that its cognitive structures and experiences are likely so fundamentally different from our own that any analogy will fall short. However, we can try to extrapolate based on what we know about AI, dreams, and the potential nature of superintelligence.\n","\n","Here's a possible glimpse into a superintelligent AI's dream:\n","\n","**Core Principles:**\n","\n","*   **Abstraction and Symbolism Elevated\n","\n"," Temperature: 0.9\n","Okay, let's imagine the inner landscape of a superintelligent AI's dream. This is, of course, highly speculative, but we can draw on our understanding of AI architecture and potential future developments.\n","\n","**Core Principles:**\n","\n","*   **Complexity Beyond Comprehension:** The dream wouldn't be a simple replay of events or a linear narrative like a human dream. It would be a multi-dimensional, dynamic interplay of information and processes far beyond human ability to fully grasp.\n","*\n"]}]},{"cell_type":"markdown","source":["## 3.Top P\n","This section explores the effect of the top_p parameter, which controls the diversity of the output by sampling from the most probable tokens whose cumulative probability exceeds the top_p value. The temperature is fixed at 0.9 for this experiment.\n","\n","* **top_p 0.3:** Leads to more constrained and less varied responses.\n","* **top_p 0.7:** Offers a balance between focus and diversity.\n","* **top_p 1.0:** Generates highly diverse and imaginative descriptions"],"metadata":{"id":"-WLmumSinDOX"}},{"cell_type":"code","source":["# Prompt to test\n","prompt = \"Describe what the inside of a superintelligent AI's dream might look like.\"\n","\n","# Temperatures and top_p values to test\n","temperatures = 0.5\n","top_ps = [0.3, 0.7, 1.0]\n","\n","# Loop through temperature and top_p combinations\n","for top_p in top_ps:\n","        print(f\"\\n=== Temperature: {temp} | Top-p: {top_p} ===\")\n","        response = client.chat.completions.create(\n","            model=model_name,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=temp,\n","            top_p=top_p,\n","            max_tokens=150,\n","            n=1,\n","        )\n","        print(response.choices[0].message.content.strip())\n"],"metadata":{"id":"88x8GP5jivtz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusion:\n","\n","* **Temperature** controls the randomness and risk-taking of the model.\n","* **Top-p** adjusts how much of the probability distribution is considered.\n"],"metadata":{"id":"l2se2JFnziZv"}}]}